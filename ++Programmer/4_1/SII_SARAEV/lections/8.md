` На прошлой лекции...
    Хоффилд предложил певые нейросети с обратными связями
    Хинтон - 2006-2016 - в 2024  получили нобелевскую премию
    (за разработку нейросететй с обратными связями)

    Искусственный нейрон является моделью биологического
    Делает простое преобразвание -
    1. этап
    скалярное произведения вектора весов (принадленжость
    вектора) и скаляра
    2. Этап
    Функция аактивации - Скалярная функция (на входе одно f а на выходе другое f)

    Существует разные функции активации
    Перцептроном будем называть тако

    Когда уровень активности нейрона не будет превышать какой-либо величины, то 0,
    иначе 1.

    Уже можно строить сложные операции с помощью комбинаций нейронов.

    Перцептрон может моделировать простые булевые функции.
    Подбирали веса таким образом, чтобы перцептрон подбирал данные, чтобы выходы перцептрона
    совпадали с выходом таблицы данных

    Подобную задачу (булева функция является функцией 2х переменных)
    можно иллюстрировать графически

    Представляет собой задание коэффициетов прямой

    Определение коэффициентов задает расподожение прямой

    Рассмотрели пример задачи, которая не может быть решена перцептроном
    xor problem - действительно нельзя подобрать такие веса перцептрона, чтобы
    по одну сторону были точки одного класса, а с другой другого класса.
`

Возвращаемся к нейрону, который может получить на вход несколько значений

Выбор весов перцептрона и порогового значения, чтобы разделить плоскость (см 5.11)

У нейрона нет доп. входного сигнала, но подразумеваем, что единица всегда подается.

<w, x> - скалярное произведение w и x

Задача линейно разделимая, если можно найти 2D отрезок или 3D+ плоскость

Если задача линейно разделимая, то можно обучить перцептрон

Предусматривается, что есть расширенный вектор весов, без порогового значения
(нейронные связи обратн) LSTM увидим, что вектор отвечающий за пороговое значение будет
вынесен отдельно
Есть вектор, отражающий состояние памяти. Вместо того, чтобы каждому присваивать вектор
смещения, используют общий вектор (объединенный)

# Алгоритм обучения перцептрона (5.12)

Задача - настроить весса перцептрона, чтобы он выдавал правильные ответы
Алгоритм состоит из нескольких этапов.
Скаляр n > 0
Кол-во строк в таблице данных k = 1
Вводим var E = 0 - ошибка работы перцептрона на обучающем множестве
Выбираем x_m и y_m из таблицы

5. Норма, потому что вектор должен быть положительной величиной

Задача обучения нейросети - задача оптимизации
Найти минимальное значение - минимизировать функцию ошибки

# Нейросети прямого распространения (5.14)

Нейросеть - совокупность взаимосвязанных нейронов
Нейросети прямого распространения (слоистые - в таких сетях нейроны организованы
в несколько слоев. Выходы нейронов из одного являются входом для другого)
MLP - multilayer perceptron

Нейроны организованы в несколько слоев. М-слойная нейросеть включает скрытый слой
и выходной слой

Слой M не имеет функцию автивации (sigma)
sigma(net) = net

В этой структуре нет обратной связи (прямое распространение)

Почему нейросети прямого распространения важны?
Сети с обратными связями представляют в виде многослойных перцептронов.
От сети с обратными связями переходим к

выход нейрона зависит от 2х индексов.
Уровень активности нейрона - скалярное произведение.

У нейрона есть определенное кол-во вектор весов и нулевой (единица).
Входы нейрона который в этом слое - выходы всех нейронов в слое m-1

W - матрица всех нейронов первого слоя
W^(1,1) - вектор весов первого слоя
W^(1,2) - вектор весов первого слоя - на вход столько же сигналов -> размерности same
W^(1,3) - вектор весов первого слоя
все это W^(1)
а если W^(1) на вектор x
получим вектор уровней активности нейронов первого слоя
net^(1)


# Двуслойная нейросеть (1 скрытый слой)

Сеть с многими выходами можно представить в виде совокупности нескольких сетей.

Глубокая нейросеть - 2+ скрытых слоя.
