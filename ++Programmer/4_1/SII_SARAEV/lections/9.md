Работаем в рамках классической теории нейросетей
Нейроны в сетях прямого распространения организованы в несколько слоев.
Особенность нейросетей прямого распространения - отсутсвтуют обратные связи.
Как называются данные нейросети?
На рисунке 14 - M-слойная нейросеть.
В наших силах выбрать - сколько будет скрытых слоев и сколько нейронов в скрытом слое.
Можно сделать так чтобы кол-во скрытых слоев -> min.
Раньше изучались нейросети с одним скрытым слоем. Теперь популярны структуры с
кол-вом скрытых слоев 10ки или 100ни.
Четкого определения глубоких нейросетей нет. Подразумевают многослойные перцептроны,
где кол-во слоев больше 1.
Чтобы выход мог получить любые числовые значения его не ограничивают -> там нет
нелинейной функции активации.
Часть весов входит линейнмы образом. Для их обучения используют частный алгоритм.
Линейные веса часто определяются псевдо-обращение(псевдо-инверсия).
Даже 1 нейрон на выходе содержит 7 параметров - весов W, которые нужно определить
(идентификация в семимерном пространстве).
Все подходы развиваются экстенсивно. Их кол-во растет очень быстро.
Увеличивая кол-во весов теоретически повышаются возможности нейросети, но с другой -
каждый вес это усложнение функции.
А какая самая простая нейросетевая структура?
самая простая сеть - простейшая нейросеть прямого распространения.
Не содержит ни одного скрытого слоя

1993 вышло 3 публикации с похожими результатами.
Если есть какая-то нейлинейная модель, то когда строим другую модель - макс.
значение отклонения модели будет > eps.

2.3
Есть способ преобразовать входные в выходные сигналы.
Это подход, который называется моделированием типа 'черный ящик'
Есть какое-то устройство, которое как-то преобразует входной сигнал
Задача идентификации - понять что внутри ящика.
Как называется заадча когда есть входные x, правила преобразования,
но выходные сигналы неизвестны - задача прогнозирования.
Задача, когда мы не знаем какие входы.
Есть обучающее множество - на вход системы подали значение строки, преобразовали,
получили значение выхода.

Цель обучения - оптимизация определения параметров.
Весь ресурс методов оптимизации можно использовать.
Если функцию нужно минимизировать, нужно искать производную^(-1).
Например, при помощи метода градиентного спуска.
Функция, которую оптимизируем, используем градиент.
Метод расчета весов перцептрона - метод наискорейшего спуска.

2.4 Метод обратного распространения ошибки - тот же самый метод вычисления градиента,
только эффективный.
